<!-- The MIT License (MIT)

Copyright ( c ) 2014,2015 Teem2 LLC

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE. -->
<!--/**
 * @class dr.microphone {UI Components}
 * @extends dr.node
 * Enables audio capture from the microphone device.
 *
 * This example uses the microphone to monitor the audio traffic, while also
 * amplifying the audio signal
 *
 *     @example
 *     <microphone id="mic" modifystream="true" visualize="false" stream="true">
 *       <method name="processStream" args="audio">
 *         if (!audio) return;
 *
 *         var inputBuffer = audio.inputBuffer;
 *         var left = inputBuffer.getChannelData(0);
 *         var right = inputBuffer.getChannelData(1);
 *
 *         var min = Math.min.apply(null, left);
 *	   var max = Math.max.apply(null, right);
 *	   console.log('min', min, 'max', max);
 *       </method>
 *
 *     </microphone>
 *
 */-->
<class name="microphone" type="coffee" clickable="true" clip="true" scriptincludes="/lib/audiorecorder/js/recorderjs/recorder.js, /lib/filesaver/FileSaver.min.js">
  
  <!--/**
    * @attribute {Boolean} [visualize=true]
    * When set to true, fft visualization data is computed and available in the fft attribute.
    */-->
  <attribute name="visualize" value="true" type="boolean"></attribute>
  <!--/**
    * @attribute {Number} fftsize
    * The number of fft frames to use when setting {@link #fft fft}. Must be a non-zero power of two in the range 32 to 2048.
    */-->
  <attribute name="fftsize" value="512" type="number"></attribute>
  <!--/**
    * @attribute {Number} [fftsmoothing=0.8]
    * The amount of smoothing to apply to the FFT analysis. A value from 0 -> 1 where 0 represents no time averaging with the last FFT analysis frame.
    */-->
  <attribute name="fftsmoothing" value="0.8" type="number"></attribute>
  <!--/**
    * @attribute {Boolean} [modifystream=false]
    * When set to true the audio stream is passed to processStream method.
    * The audio object has an inputBuffer and outputBuffer. If you pass the
    * inputBuffer data to the outputBuffer, the audio will be passed to the
    * speaker.
    */-->
  <attribute name="modifystream" value="false" type="boolean"></attribute>
  <!--/**
    * @attribute {Boolean} [stream=false]
    * When set to true the user will be asked permission to access the device, and if permission is granted the stream will be started. When set to false the stream will be stopped. The {@link #streaming streaming} attribute indicates when the device is actually streaming.
    */-->
  <attribute name="stream" type="boolean" value="false"></attribute>
  <setter name="stream" args="stream">
    if (stream)
      @startStream()
    else
      @stopStream()
    
    return stream
  </setter>
  <!--/**
    * @attribute {Boolean} [record=false]
    * Set this to true to started recording the input stream, set to false to stop. When recording is stopped the recording will be retained in memory until another recording is started. The URL to access the recording is saved in the playbackurl attribute. The {@link #recording recording} attribute indicates when the device is actually recording.  
    */-->
  <attribute name="record" type="boolean" value="false"></attribute>
  <setter name="record" args="record">
    if (record)
      @startRecording()
    else
      @stopRecording()

    return record
  </setter>
  
  <!--/**
    * @attribute {LocalMediaStream} mediastream
    * @readonly
    * The input stream from the microphone.
    */-->
  <attribute name="mediastream" type="expression" value="null"/>
  <!--/**
    * @attribute {Boolean} [streaming=false]
    * @readonly
    * True when input is streaming from the device. False when it has been stopped, or if the user has denied access to the device.
    */-->
  <attribute name="streaming" type="boolean" value="false"/>
  <!--/**
    * @attribute {Boolean} [recording=false]
    * @readonly
    * True when the stream is being recorded.
    */-->
  <attribute name="recording" type="boolean" value="false"/>
  <!--/**
    * @attribute {Boolean} [permissiondenied=false]
    * @readonly
    * True if the user has denied permission to access the device.
    */-->
  <attribute name="permissiondenied" type="boolean" value="false"/>
  <!--/**
    * @attribute {Number[]} fft
    * @readonly
    * An array of numbers representing the FFT analysis of the audio as it's playing.
    */-->
  <attribute name="fft" value="[]" type="expression"></attribute>
  <!--/**
    * @attribute {String} playbackurl
    * @readonly
    * Object URL of the latest recording, which can be used with an instance of dr.audioplayer to play back the recording.
    */-->
  <attribute name="playbackurl" type="string" value=""></attribute>
  <!--/**
    * @attribute {Boolean} [supported=true]
    * @readonly
    * False if the microphone component is not supported in the users browser. 
    */-->
  <attribute name="supported" type="boolean" value="true"></attribute>
  <!--/**
    * @attribute {Number} recordingtime
    * @readonly
    * The length of the recording in seconds. 
    */-->
  <attribute name="recordingtime" type="number" value="0"></attribute>

  <method name="construct" args="element, attributes">
    window.AudioContext = window.AudioContext || window.webkitAudioContext

    navigator.getUserMedia = navigator.getUserMedia ||
      navigator.webkitGetUserMedia ||
      navigator.mozGetUserMedia ||
      navigator.msGetUserMedia

    attributes['supported'] = !!navigator.getUserMedia
    
    @super(element, attributes)
  </method>

  <handler event="oninit">
    @startStream(!!'forceStart') if @stream
  </handler>

  <!--<method name="isMuted">-->
    <!--return false unless @mediastream && @mediastream.getAudioTracks()[0]-->
    <!--return @mediastream.getAudioTracks()[0].muted-->
  <!--</method>-->

  <!--/**
    * @method processStream
    * Process raw audio data from the microphone.
    * audioprocess objects are passed that contain stereo data samples.
    * The default implementation copies the input buffer into the output buffer.
    * @param {Object} audio audioprocess object.
    */-->
  <method name="processStream" args="audio">
    inputBuffer = audio.inputBuffer
    outputBuffer = audio.outputBuffer
    left = inputBuffer.getChannelData(0)
    right = inputBuffer.getChannelData(1)
    oleft = outputBuffer.getChannelData(0)
    oright = outputBuffer.getChannelData(1)

    for i in [0..left.length] by 1
      oleft[i] = left[i]
      oright[i] = right[i]
    #console.log('processStream', left.length)
  </method>

  <method name="startStream" args="forceStart">
    return unless @inited
    return if @stream && !forceStart

    errorCallback = (e) =>
      console.log('User denied permission to access the microphone', e)
      @setAttribute('permissiondenied', true)
      @setAttribute('stream', false) ####reset this so they can try again

    gotLocalMediaStream = (localMediaStream) =>
      audioContext = new AudioContext()
      @_inputGain = audioContext.createGain()

      audioInput = audioContext.createMediaStreamSource(localMediaStream)
      audioInput.connect(@_inputGain)

      # FFT visualization
      if @visualize
        @_fftNode = audioContext.createAnalyser()
        @_fftNode.fftSize = @fftsize
        @_inputGain.connect(@_fftNode)
        @_fftNode.smoothingTimeConstant = @fftsmoothing

      # Raw results
      if @modifystream
        # Define number of stereo samples to buffer for each callback
        @_bufferSize = 1024
        @_rawstream = audioContext.createScriptProcessor(@_bufferSize, 2, 2)

        @_rawstream.onaudioprocess = (audio) =>
          @processStream(audio)

        @_inputGain.connect(@_rawstream)
        @_rawstream.connect(audioContext.destination)
      
      @setAttribute('streaming', true)
      @setAttribute('mediastream', localMediaStream)
    
      #in case stream was set to false before we get this callback
      @stopStream() unless @stream

      @startRecording() if (@stream && @record)
    
    navigator.getUserMedia({video: false, audio: true}, gotLocalMediaStream, errorCallback)
  </method>

  <method name="stopStream">
    return unless @streaming
    
    for track in @mediastream.getAudioTracks()
      track.stop();
    @mediastream.stop();

    @_inputGain = null
    @_fftNode = null
    @_rawstream.onaudioprocess = null if @_rawstream
    @_rawstream = null
    
    @setAttribute('streaming', false)
  </method>
  
  <method name="startRecording">
    return unless @streaming && !@recording

    unless @_audioRecorder
      @_audioRecorder = new Recorder(@_inputGain, {workerPath:'/lib/audiorecorder/js/recorderjs/recorderWorker.js'})

    @clearRecordingData() if @playbackurl
    @_audioRecorder.record()

    @_startTime = new Date()
    @setAttribute('recording', true)
  </method>
  
  <method name="stopRecording">
    return unless @recording
    
    @_audioRecorder.stop()

    @_audioRecorder.exportWAV (blob) =>
      @setAttribute('playbackurl', (window.URL || window.webkitURL).createObjectURL(blob))

    @setAttribute('recording', false)
  </method>
  
  <method name="clearRecordingData">
    (window.URL || window.webkitURL).revokeObjectURL(@playbackurl);
    @setAttribute('playbackurl', '')
    @setAttribute('recordingtime', 0)
    @_audioRecorder.clear() if @_audioRecorder
  </method>

  <handler event="onidle" reference="dr.idle">
    return unless @streaming

    if @recording
      currentTime = new Date();
      @setAttribute('recordingtime', (currentTime - @_startTime)/1000)

    if @_fftNode
      data = new Uint8Array(@_fftNode.frequencyBinCount)
      @_fftNode.getByteFrequencyData(data)
      @setAttribute('fft', data)
  </handler>
  
  <method name="downloadRecording" args="fileName">
    @_audioRecorder.exportWAV (blob) =>
      saveAs(blob, fileName + ".wav")
  </method>
</class>
